{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patricknormile/small-language-model/slm-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "import mlflow\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformer_lens import utils, HookedTransformer\n",
    "from transformers import (\n",
    "    GPT2Tokenizer, \n",
    "    GPT2LMHeadModel, \n",
    "    DataCollatorForLanguageModeling, \n",
    "    Trainer, \n",
    "    TrainingArguments\n",
    ")\n",
    "import mlflow\n",
    "from transformers.integrations import MLflowCallback\n",
    "\n",
    "from create_dataset import make_rows_from_chat\n",
    "\n",
    "file_name = \"artifacts/input_text.txt\"\n",
    "\n",
    "with open(file_name, \"r\") as file_read:\n",
    "    chat = file_read.read()\n",
    "\n",
    "chat_rows = make_rows_from_chat(chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'target'],\n",
       "    num_rows: 62125\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "def make_generator(data, tokenizer) :\n",
    "    for x in data : \n",
    "        tokens = tokenizer(x, truncation=True, padding=True)['input_ids']\n",
    "        for i in range(1,len(tokens), 1) :\n",
    "            x, y = tokens[:i], tokens[i]\n",
    "            yield {\"text\":tokenizer.decode(x), \"target\":tokenizer.decode(y)}\n",
    "ds = Dataset.from_generator(make_generator,gen_kwargs={'data':chat_rows[-3000:],\n",
    "'tokenizer':tokenizer})\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, would you mind\n",
      "Hello, would you mind if\n",
      "Hello, would you mind if I\n",
      "Hello, would you mind if I could\n",
      "Hello, would you mind if I could take\n",
      "Hello, would you mind if I could take a\n",
      "Hello, would you mind if I could take a moment\n",
      "Hello, would you mind if I could take a moment to\n",
      "Hello, would you mind if I could take a moment to explain\n",
      "Hello, would you mind if I could take a moment to explain to\n",
      "Hello, would you mind if I could take a moment to explain to you\n",
      "Hello, would you mind if I could take a moment to explain to you how\n",
      "Hello, would you mind if I could take a moment to explain to you how I\n",
      "Hello, would you mind if I could take a moment to explain to you how I got\n",
      "Hello, would you mind if I could take a moment to explain to you how I got here\n",
      "Hello, would you mind if I could take a moment to explain to you how I got here?\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param, name in zip(model.parameters(),model.named_parameters()) :\n",
    "    if not re.match(\"^transformer.h.11.\",name[0]) :\n",
    "        continue\n",
    "    param.requires_grad = True\n",
    "\n",
    "input_text = \"Hello, would you mind\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "def next_token(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "    # Extract logits for the last token\n",
    "    logits = outputs.logits\n",
    "    last_token_logits = logits[0, -1, :]\n",
    "\n",
    "    # Apply softmax to get probabilities\n",
    "    probabilities = torch.softmax(last_token_logits, dim=-1)\n",
    "\n",
    "    # Find the predicted token id\n",
    "    predicted_token_id = torch.argmax(probabilities).item()\n",
    "\n",
    "    # Decode the predicted token to get the word\n",
    "    predicted_token = tokenizer.decode([predicted_token_id])\n",
    "    return predicted_token\n",
    "\n",
    "print(input_text)\n",
    "for i in range(15):\n",
    "    add = next_token(input_text)\n",
    "    input_text += add\n",
    "    print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/17475 [1:48:35<3161:06:24, 651.59s/it]\n",
      "  0%|          | 0/34945 [1:17:01<?, ?it/s]\n",
      "  1%|          | 200/34945 [26:45<678:57:20, 70.35s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.9893, 'grad_norm': 0.6418070197105408, 'learning_rate': 0.0009942767205608815, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 400/34945 [1:33:09<21:10:35,  2.21s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0992, 'grad_norm': 0.9630653858184814, 'learning_rate': 0.000988553441121763, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 600/34945 [2:41:10<38:41:33,  4.06s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.2117, 'grad_norm': nan, 'learning_rate': 0.0009828301616826441, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 800/34945 [2:51:25<27:51:54,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0009771068822435256, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 862/34945 [3:22:02<148:56:08, 15.73s/it]  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mlflow\u001b[38;5;241m.\u001b[39mstart_run():\n\u001b[0;32m---> 42\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     eval_metrics \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate(eval_dataset\u001b[38;5;241m=\u001b[39mtest_ds)\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m eval_metrics\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/small-language-model/slm-env/lib/python3.11/site-packages/transformers/trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1858\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1859\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1860\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1861\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1862\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1863\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1864\u001b[0m     )\n",
      "File \u001b[0;32m~/small-language-model/slm-env/lib/python3.11/site-packages/transformers/trainer.py:2219\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2214\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2215\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCalculated loss must be on the original device: \u001b[39m\u001b[39m{\u001b[39;00mtr_loss\u001b[39m.\u001b[39mdevice\u001b[39m}\u001b[39;00m\u001b[39m but device in use is \u001b[39m\u001b[39m{\u001b[39;00mtr_loss_step\u001b[39m.\u001b[39mdevice\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2216\u001b[0m         )\n\u001b[1;32m   2217\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss_step\n\u001b[0;32m-> 2219\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_flos \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfloating_point_ops(inputs))\n\u001b[1;32m   2221\u001b[0m is_last_step_and_steps_less_than_grad_acc \u001b[39m=\u001b[39m (\n\u001b[1;32m   2222\u001b[0m     steps_in_epoch \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m args\u001b[39m.\u001b[39mgradient_accumulation_steps \u001b[39mand\u001b[39;00m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m steps_in_epoch\n\u001b[1;32m   2223\u001b[0m )\n\u001b[1;32m   2225\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   2226\u001b[0m     total_batched_samples \u001b[39m%\u001b[39m args\u001b[39m.\u001b[39mgradient_accumulation_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m   2227\u001b[0m     \u001b[39mor\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2231\u001b[0m     \u001b[39m# the `or` condition of `is_last_step_and_steps_less_than_grad_acc` is not covered\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     \u001b[39m# in accelerate. So, explicitly enable sync gradients to True in that case.\u001b[39;00m\n",
      "File \u001b[0;32m~/small-language-model/slm-env/lib/python3.11/site-packages/transformers/trainer.py:3864\u001b[0m, in \u001b[0;36mTrainer.floating_point_ops\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3860\u001b[0m         logits \u001b[39m=\u001b[39m logits[\u001b[39m0\u001b[39m]\n\u001b[1;32m   3862\u001b[0m     \u001b[39mreturn\u001b[39;00m (loss, logits, labels)\n\u001b[0;32m-> 3864\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfloating_point_ops\u001b[39m(\u001b[39mself\u001b[39m, inputs: Dict[\u001b[39mstr\u001b[39m, Union[torch\u001b[39m.\u001b[39mTensor, Any]]):\n\u001b[1;32m   3865\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3866\u001b[0m \u001b[39m    For models that inherit from [`PreTrainedModel`], uses that method to compute the number of floating point\u001b[39;00m\n\u001b[1;32m   3867\u001b[0m \u001b[39m    operations for every backward + forward pass. If using another model, either implement such a method in the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3875\u001b[0m \u001b[39m        `int`: The number of floating-point operations.\u001b[39;00m\n\u001b[1;32m   3876\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   3877\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39m\"\u001b[39m\u001b[39mfloating_point_ops\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=True, return_tensors='pt')\n",
    "\n",
    "tokenized_datasets = ds.map(tokenize_function, batched=True)\n",
    "\n",
    "split_ds = tokenized_datasets.train_test_split(test_size=0.1)\n",
    "train_ds = split_ds[\"train\"]\n",
    "test_ds = split_ds[\"test\"]\n",
    "\n",
    "model.to(\"mps\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,  # Adjust batch size as needed\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=0.001,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=200,\n",
    "    report_to=\"none\"  # Disable reporting to W&B\n",
    ")\n",
    "\n",
    "# Set up MLflow\n",
    "mlflow.set_tracking_uri(\"./outputs\")  # Replace with your tracking URI\n",
    "mlflow.set_experiment(\"gpt2_fine_tuning\")\n",
    "\n",
    "# Initialize Trainer with MLflow callback\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[MLflowCallback()],\n",
    ")\n",
    "\n",
    "# Start training\n",
    "with mlflow.start_run():\n",
    "    trainer.train()\n",
    "    eval_metrics = trainer.evaluate(eval_dataset=test_ds)\n",
    "    for key, value in eval_metrics.items():\n",
    "        mlflow.log_metric(key, value)\n",
    "    mlflow.pyfunc.log_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"]=\"0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, would you mind\n",
      "Hello, would you mind!\n",
      "Hello, would you mind!!\n",
      "Hello, would you mind!!!\n",
      "Hello, would you mind!!!!\n",
      "Hello, would you mind!!!!!\n",
      "Hello, would you mind!!!!!!\n",
      "Hello, would you mind!!!!!!!\n",
      "Hello, would you mind!!!!!!!!\n",
      "Hello, would you mind!!!!!!!!!\n",
      "Hello, would you mind!!!!!!!!!!\n",
      "Hello, would you mind!!!!!!!!!!!\n",
      "Hello, would you mind!!!!!!!!!!!!\n",
      "Hello, would you mind!!!!!!!!!!!!!\n",
      "Hello, would you mind!!!!!!!!!!!!!!\n",
      "Hello, would you mind!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "# see how it changed\n",
    "model.to(\"cpu\")\n",
    "input_text = \"Hello, would you mind\"\n",
    "print(input_text)\n",
    "for i in range(15):\n",
    "    add = next_token(input_text)\n",
    "    input_text += add\n",
    "    print(input_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 ('slm-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fbbb8e684d57b8df141df854a0f38ddd4615731c6e64784ef39c5a6ebd7fa864"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
